function nn_output = feedforward(dip_state)
%This function calculates output of neural network
%
nodesCount = [6, 10, 1];

weights{1} = [ 
-0.43959682378620135,1.8627066942659487,1.9786260558087903,1.3446390902649812,1.062372229188191,0.5017284123833894
2.579229974714869,-1.8654653505329795,3.69203925736623,3.8699261521707586,-1.0904049672256015,-4.2725725893999495
3.732901467825183,-1.4899916095007715,0.3204255747054375,0.3300518417205374,2.5867589477541886,-1.3089150136207743
3.7714215997388507,-1.9533952569649113,2.4920149311920734,-0.9227633295681088,2.0324522366621887,-0.12485407029682735
0.39650519044092447,-0.4886972946504702,-1.9059333808290946,1.7429585067010303,-2.3817419355949907,1.4750925365282723
-2.2480545911520338,-3.2126285072427434,-0.1619981599135691,1.9674485642055002,-1.482410989187944,1.6316110205640653
-1.2747352845762752,0.8825192787637914,0.7422410866448751,-1.3349239320460584,-3.159395205459748,-0.39320169133167876
1.0189352461095005,0.46101826765102905,0.09576062063512837,2.7880192555016334,0.4405576305359703,0.37590261000063174
-2.294960674423295,-2.3822083717143596,1.588253627025626,1.3584402281039587,2.09080422211177,-3.34239368763377
0.786192494437817,-2.779842598726815,1.6748876950471545,0.17318981568804903,4.212803921112894,1.0707817078173585
];

weights{2} = [-0.4213494781970466,-1.289752632014824,2.131147137055913,3.0013040025302273,0.7776877552461124,-4.9130609416025965,0.33218474305050627,-0.9623169375270323,2.182771260380689,3.398312751722844];

biases{1} = [0.006645937727594428,0.2704243518622918,1.6027427386847641,-0.830943744533214,-2.3572969492183304,-0.5222825618402926,-0.6020729386028985,-1.5685247612411921,-1.7327961861427545,-0.5496768945296635];
biases{2} = [1.2425585686811251];

layer2 = zeros(nodesCount(2));
layer3 = zeros(nodesCount(3));

layers = {dip_state, layer2, layer3};

for ll = 1:(length(layers) - 1)
    for ii = 1:nodesCount(ll + 1)
        temp = 0;
        for jj = 1:nodesCount(ll)  
            temp = temp + layers{ll}(jj) * weights{ll}(ii, jj);
        end
        layers{ll+1}(ii) = temp + biases{ll}(ii);
    end
    % activation
    if(ll ~= (length(layers) - 1))
        % ReLU activation
        for nn = 1:length(layers{ll+1})
            %fprintf('ReLU: %i, %i\n', ll, nn);
            if layers{ll+1}(nn) < 0
                layers{ll+1}(nn) = 0;
            end
        end
    else
        % logistic actication
        for nn = 1:length(layers{ll+1})
            %fprintf('logistic: %i, %i\n', ll, nn);
            layers{ll+1}(nn) = 1 / (1 + exp(-layers{ll+1}(nn)));
        end
    end
end
nn_output = layers{end}; 
end